{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 토큰화\n",
    "## 1-1. casual_tokenize 토큰화, stopwords로 단어 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "df = pd.read_csv('Warframe_reviews.csv')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed_tokens_list = []\n",
    "\n",
    "for text in df['review_text']:\n",
    "    text = str(text)\n",
    "    \n",
    "    # 토큰화\n",
    "    tokens = casual_tokenize(text, reduce_len=True, strip_handles=True)\n",
    "    \n",
    "    # nltk.stop_word로 단어 삭제\n",
    "    filtered_tokens = [t for t in tokens if len(t) > 1 and t.lower() not in stop_words]\n",
    "    \n",
    "    processed_tokens_list.append(filtered_tokens)\n",
    "    \n",
    "print(processed_tokens_list)\n",
    "\n",
    "# 각 리뷰의 토큰들을 모두 모으기\n",
    "all_tokens = [word for tokens in processed_tokens_list for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 품사 태깅\n",
    "+ game, play 등의 의미없는 단어가 너무 많음. => 일일히 걸러주자\n",
    "+ 품사 태깅 이유 : 키워드 추출 할때 부사와 같은 꾸며주는 단어가 많이 나와서 추출하는데 의미가 없다고 생각이 들었음\n",
    "+ 동사, 명사와 같은 의미있는 단어만 뽑아주기 위해 품사 태깅 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅 테스트 (동사 많이 나온 단어 추출)\n",
    "# from collections import Counter\n",
    "\n",
    "# tagged_tokens = nltk.pos_tag(all_tokens)\n",
    "# nouns = [word for word, pos in tagged_tokens if pos.startswith('VB')]\n",
    "\n",
    "# counter = Counter(nouns)\n",
    "# top_30_nn = counter.most_common(30)\n",
    "# print(top_30_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 품사 정의\n",
    "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']\n",
    "\n",
    "tagged_tokens = nltk.pos_tag(all_tokens)\n",
    "print(tagged_tokens)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# most_common() : Counter 객체에서 빈도수가 높은 순서대로 데이터를 정렬한 리스트 반환\n",
    "print(Counter(tagged_tokens).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리: 특정 품사 태그에 해당하는 단어만 필터링\n",
    "stop_words = [word for word, tag in tagged_tokens if tag in stopPos]\n",
    "filtered_tokens = [token for token in all_tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. 정규화 (원형 복원)\n",
    "### 1-3-1. 소문자 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import normal\n",
    "\n",
    "# lower메소드로 정규화 \n",
    "normalized_tokens = [x.lower() for x in filtered_tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3-2. 어간 추출 (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(word) for word in all_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3-3. 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')  # 선택: 시소러스(동의어) 관련\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_text = ['game', 'warframe', 'destiny', 'destiny2', 'thefirstdescendant', 'gameplay', 'descendant', 'tfd', 'first', 'play']\n",
    "\n",
    "filtered_tokens = [tokens for tokens in lemmatized_tokens if tokens not in stop_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Jina-Embeddings-v3 모델 로드\n",
    "model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "# 사용할 태스크 설정 (예: 문장 유사도 계산을 위한 임베딩)\n",
    "task = \"text-matching\"  \n",
    "\n",
    "df = pd.read_csv('The_first_descendant_positive_reviews.csv')\n",
    "df['review_text'] = df[\"review_text\"].fillna(\"\").astype(str)\n",
    "\n",
    "\n",
    "# review_text 열을 리스트로 변환\n",
    "sentences = filtered_tokens\n",
    "\n",
    "# 문장 임베딩 생성\n",
    "embeddings = model.encode(\n",
    "    sentences,\n",
    "    task=task,         # 태스크 지정 (검색용 쿼리 임베딩)\n",
    "    prompt_name=task,   # 프롬프트 지정\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(embeddings.shape)  # 예: (3, 768) -> 3개의 문장이 768차원 벡터로 변환됨"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
